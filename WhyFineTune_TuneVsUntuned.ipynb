{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNylgRK7ruBLlKol4pI/kG0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carlos1729/Transformers_Code/blob/main/WhyFineTune_TuneVsUntuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Authenticate with Google\n",
        "# @markdown Note: You will be asked to sign in with Google, connected to your Lamini account.\n",
        "\n",
        "from google.colab import auth\n",
        "import requests\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "def authenticate_powerml():\n",
        "  auth.authenticate_user()\n",
        "  gcloud_token = !gcloud auth print-access-token\n",
        "  powerml_token_response = requests.get('https://api.powerml.co/v1/auth/verify_gcloud_token?token=' + gcloud_token[0])\n",
        "  print(powerml_token_response)\n",
        "  return powerml_token_response.json()['token']\n",
        "\n",
        "key = authenticate_powerml()\n",
        "\n",
        "config = {\n",
        "    \"production\": {\n",
        "        \"key\": key,\n",
        "        \"url\": \"https://api.powerml.co\"\n",
        "    }\n",
        "}\n",
        "\n",
        "keys_dir_path = '/root/.powerml'\n",
        "os.makedirs(keys_dir_path, exist_ok=True)\n",
        "\n",
        "keys_file_path = keys_dir_path + '/configure_llama.yaml'\n",
        "with open(keys_file_path, 'w') as f:\n",
        "  yaml.dump(config, f, default_flow_style=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf7jetitKwkC",
        "outputId": "2e1b0740-8bae-4770-90a3-62774ad8792d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JirCt_4wGqX0",
        "outputId": "c0808fd5-ae33-4d83-d3dd-0d5267280cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lamini\n",
            "  Downloading lamini-1.0.2-23-py3-none-any.whl (60 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m51.2/60.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==1.10.* (from lamini)\n",
            "  Using cached pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Collecting lamini-configuration[yaml] (from lamini)\n",
            "  Using cached lamini_configuration-0.8.3-py3-none-any.whl (22 kB)\n",
            "Collecting requests (from lamini)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Collecting tokenizers (from lamini)\n",
            "  Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "Collecting tqdm (from lamini)\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Collecting numpy (from lamini)\n",
            "  Using cached numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Collecting scipy (from lamini)\n",
            "  Using cached scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "Collecting scikit-learn (from lamini)\n",
            "  Using cached scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "Collecting jsonlines (from lamini)\n",
            "  Using cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting pandas (from lamini)\n",
            "  Using cached pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "Collecting bs4 (from lamini)\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu (from lamini)\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.2.0 (from pydantic==1.10.*->lamini)\n",
            "  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting beautifulsoup4 (from bs4->lamini)\n",
            "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=19.2.0 (from jsonlines->lamini)\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting pyyaml<7.0,>=6.0 (from lamini-configuration[yaml]->lamini)\n",
            "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->lamini)\n",
            "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->lamini)\n",
            "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas->lamini)\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->lamini)\n",
            "  Using cached charset_normalizer-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->lamini)\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->lamini)\n",
            "  Using cached urllib3-2.0.6-py3-none-any.whl (123 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->lamini)\n",
            "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "Collecting joblib>=1.1.1 (from scikit-learn->lamini)\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn->lamini)\n",
            "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Collecting huggingface_hub<0.18,>=0.16.4 (from tokenizers->lamini)\n",
            "  Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "Collecting filelock (from huggingface_hub<0.18,>=0.16.4->tokenizers->lamini)\n",
            "  Using cached filelock-3.12.4-py3-none-any.whl (11 kB)\n",
            "Collecting fsspec (from huggingface_hub<0.18,>=0.16.4->tokenizers->lamini)\n",
            "  Using cached fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
            "Collecting packaging>=20.9 (from huggingface_hub<0.18,>=0.16.4->tokenizers->lamini)\n",
            "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->lamini)\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->bs4->lamini)\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=7bbce63853a838f2ceceb258a42f07864e1ab3f92015ccf787eab773e89d0d3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: pytz, faiss-cpu, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, soupsieve, six, pyyaml, packaging, numpy, lamini-configuration, joblib, idna, fsspec, filelock, charset-normalizer, certifi, attrs, scipy, requests, python-dateutil, pydantic, jsonlines, beautifulsoup4, scikit-learn, pandas, huggingface_hub, bs4, tokenizers, lamini\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.1 which is incompatible.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.9.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.1 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.1 which is incompatible.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 beautifulsoup4-4.11.2 bs4-0.0.1 certifi-2023.7.22 charset-normalizer-3.3.0 faiss-cpu-1.7.4 filelock-3.12.4 fsspec-2023.6.0 huggingface_hub-0.17.3 idna-3.4 joblib-1.3.2 jsonlines-4.0.0 lamini-0.0.19 lamini-configuration-0.8.3 numpy-1.23.5 packaging-23.2 pandas-1.5.3 pydantic-1.10.13 python-dateutil-2.8.2 pytz-2023.3.post1 pyyaml-6.0.1 requests-2.31.0 scikit-learn-1.2.2 scipy-1.11.3 six-1.16.0 soupsieve-2.5 threadpoolctl-3.2.0 tokenizers-0.14.1 tqdm-4.66.1 typing-extensions-4.5.0 tzdata-2023.3 urllib3-2.0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "charset_normalizer",
                  "dateutil",
                  "joblib",
                  "jsonlines",
                  "llama",
                  "numpy",
                  "pandas",
                  "pydantic",
                  "pytz",
                  "requests",
                  "six",
                  "sklearn",
                  "tqdm",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Step 2: Install the open-source [Lamini library](https://pypi.org/project/lamini/) to use LLMs easily\n",
        "# @markdown Note: After installing, click the \"RESTART RUNTIME\" button at the end of the output, then go onto the next cell.\n",
        "# @markdown Lamini is just on a more recent version of numpy than Colab.\n",
        "!pip install --upgrade --force-reinstall --ignore-installed lamini"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LqxyHW_zQRgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYIa4RneQRzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama import LlamaV2Runner\n",
        "\n",
        "model = LlamaV2Runner()"
      ],
      "metadata": {
        "id": "5dLuNclYKcbT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama import BasicModelRunner"
      ],
      "metadata": {
        "id": "eF9UHwu0Gww_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_finetuned = BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")"
      ],
      "metadata": {
        "id": "16X0vtTtGwz1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_finetuned_output = non_finetuned(\"Tell me how to train my dog to sit\")"
      ],
      "metadata": {
        "id": "khgs6-DWGw3B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr3m3QcWGw6E",
        "outputId": "647d354e-9caf-4416-b88c-4ef679322fee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "Tell me how to train my dog to sit. I have a 10 month old puppy and I want to train him to sit. I have tried the treat method and he just sits there and looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"What do you think of Mars?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq86YixIGw84",
        "outputId": "84a50b2a-132e-4f4b-9e0d-dbe08466c163"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I think it's a great planet.\n",
            "I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think it's a great planet. I think\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"taylor swift's best friend\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9nrFcTZGxAE",
        "outputId": "bfd2e062-c091-411d-a67d-f9d3c18bbc8e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I'm not sure if I've ever mentioned this before, but I'm a huge Taylor Swift fan. I've been a fan since her first album, and I've been a fan ever since. I've been a fan of her music, her lyrics, her style, her personality, and her personality. I've been a fan of her music, her lyrics, her style, her personality, and her personality. I've been a fan of her music, her lyrics, her style, her personality, and her personality. I've been a fan of her music, her lyrics, her style, her personality, and her personality. I've been a fan of her music, her lyrics, her style, her personality, and her personality. I've been a fan of her music, her lyrics, her style, her personality, and her personality. I've been a fan of her music, her lyrics, her style, her personality, and her personality. I've been a fan of her music, her lyrics, her style, her personality, and her personality. I'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf4uki1jRAPA",
        "outputId": "fab07d0a-7ca3-4cd5-cccd-0053ac74cbf8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I'm sorry to hear that. I'll look into it.\n",
            "Customer: I'm not sure if I got the right blanket.\n",
            "Agent: I'm sorry to hear that. I'll look into it.\n",
            "Customer: I'm not sure if I got the right blanket. I'm not sure if I got the right blanket.\n",
            "Agent: I'm sorry to hear that. I'll look into it. I'll look into it.\n",
            "Customer: I'm not sure if I got the right blanket. I'm not sure if I got the right blanket. I'm not sure if I got the right blanket.\n",
            "Agent: I'm sorry to hear that. I'll look into it. I'll look into it. I'll look into it.\n",
            "Customer: I'm not sure if I got the right blanket. I'm not sure if I got the right blanket. I'm not sure if I got the right blanket. I'm not sure if I got the right blanket. I'm not sure if I got the right blanket. I'm not sure if I got the right blank\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model = BasicModelRunner(\"meta-llama/Llama-2-7b-chat-hf\")"
      ],
      "metadata": {
        "id": "dRygND-2RAR6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_output = finetuned_model(\"Tell me how to train my dog to sit\")"
      ],
      "metadata": {
        "id": "U_yX4QCBRAVA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HIbXnibRAYb",
        "outputId": "ff37f4a3-8c1f-4da7-fcb0-3c2764314f28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " on command.\n",
            "How to Train Your Dog to Sit on Command\n",
            "Training your dog to sit on command is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here's a step-by-step guide on how to train your dog to sit on command:\n",
            "1. Choose a Quiet and Distraction-Free Area: Find a quiet area with minimal distractions where your dog can focus on you.\n",
            "2. Have Treats Ready: Choose your dog's favorite treats and have them ready to use as rewards.\n",
            "3. Stand in Front of Your Dog: Stand in front of your dog and hold a treat close to their nose.\n",
            "4. Move the Treat Above Your Dog's Head: Slowly move the treat above your dog's head, towards their tail. As your dog follows the treat with their nose, their bottom will naturally lower into a sitting position.\n",
            "5. Say \"Sit\" and Reward: As soon as your dog's butt touches the ground, say \"Sit\" and give them the treat. It's important to say the command word as they're performing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And the way to actually, quote unquote,\n",
        "# get rid of this extra auto-complete thing is actually to\n",
        "# inform the model that you want instructions.\n",
        "# So I'm actually putting these instruction tags here.\n",
        "# This was used for LLAMA2."
      ],
      "metadata": {
        "id": "e5V5ADy6GxDn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"[INST]Tell me how to train my dog to sit[/INST]\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W9KaeXWSPdX",
        "outputId": "5c298385-5887-4d1e-92af-822307c28c78"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Training your dog to sit is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here's a step-by-step guide on how to train your dog to sit:\n",
            "1. Choose a quiet and distraction-free area: Find a quiet area with no distractions where your dog can focus on you.\n",
            "2. Have treats ready: Choose your dog's favorite treats and have them ready to use as rewards.\n",
            "3. Stand in front of your dog: Stand in front of your dog and hold a treat close to their nose.\n",
            "4. Move the treat up and back: Slowly move the treat up and back, towards your dog's tail, while saying \"sit\" in a calm and clear voice.\n",
            "5. Dog will sit: As you move the treat, your dog will naturally sit down to follow the treat. The moment their bottom touches the ground, say \"good sit\" and give them the treat.\n",
            "6. Repeat the process: Repeat steps 3-5 several times, so your dog starts to associate the command \"sit\" with the action of sitting down.\n",
            "7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"[INST]Tell me how to train my dog to sit[/INST]\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8pfV7C-SPgV",
        "outputId": "3bc21049-5cdb-42fc-9cc7-2330b0681797"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INST]Tell me how to train my dog to sit[/INST]\n",
            "[INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"What do you think of Mars?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLZK2MdBSPjV",
        "outputId": "71a70f24-4f0a-48f7-b2de-22076cb9bfe0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Mars is a fascinating planet that has captured the imagination of humans for centuries. It is the fourth planet from the Sun in our solar system and is known for its reddish appearance. Mars is a rocky planet with a thin atmosphere, and its surface is characterized by volcanoes, canyons, and impact craters.\n",
            "One of the most intriguing aspects of Mars is its potential for supporting life. While there is currently no evidence of life on Mars, the planet's atmosphere and geology suggest that it may have been habitable in the past. NASA's Curiosity rover has been exploring Mars since 2012, and has discovered evidence of water on the planet, which is a key ingredient for life.\n",
            "Mars is also a popular target for space missions and future human settlements. Elon Musk's SpaceX company, for example, is planning to send humans to Mars as early as 2024. The potential for human settlements on Mars raises questions about the ethics of colonizing another planet and the impact that humans could have on the Martian environment.\n",
            "Overall, Mars is a fascinating and complex planet that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"taylor swift's best friend\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTGQX_k5SPmJ",
        "outputId": "77ed40c5-cb21-4b2e-bb83-7854494b8732"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Taylor Swift's best friend is a person who has been by her side through thick and thin. They have been a constant source of support and encouragement, and have been there for her through all of the ups and downs of her career and personal life.\n",
            "\n",
            "Taylor Swift's best friend is someone who is not afraid to speak their mind and tell her when she is being silly or stubborn. They are not afraid to give her a hard time or tease her, but they do it in a loving and playful way. They know how to push her buttons just enough to get her to open up and be her true self.\n",
            "\n",
            "Taylor Swift's best friend is someone who is fiercely loyal and protective of her. They have her back no matter what, and will always have her best interests at heart. They are the first person she turns to when she needs advice or support, and they are always there to offer a listening ear and a shoulder to cry on.\n",
            "\n",
            "Taylor Swift's best friend is someone who is not afraid to be themselves around her. They don't try to be someone they're not in order to impress her, and they don't put on air\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPHyKQUnSPpO",
        "outputId": "d229e955-b019-45b4-f50c-2828fa384ed8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I see. Can you please provide me with your order number so I can look into this for you?\n",
            "Customer: I don't have the order number.\n",
            "Agent: Okay, no worries. Can you please tell me the name of the item you didn't receive?\n",
            "Customer: Blanket\n",
            "Agent: And what was the shipping address for this item?\n",
            "Customer: 123 Main St\n",
            "Agent: Okay, thank you for providing that information. I'm going to check on the status of your order for you. Can you please hold for just a moment?\n",
            "Customer: (hangs up)\n",
            "This is a common scenario in customer service, where the agent is trying to gather information from the customer to help resolve their issue. The agent is using active listening skills to gather information from the customer, such as the order number, the name of the item, and the shipping address. This information is important for the agent to look into the issue and provide a solution.\n",
            "In this scenario, the agent is using a friendly and empathetic tone to communicate with the customer. They are also being patient and taking the time to listen to the customer's concern, which can help to build trust and rapport with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Compare to ChatGPT"
      ],
      "metadata": {
        "id": "hxzvjOigSPsr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt = BasicModelRunner(\"chat-gpt\")"
      ],
      "metadata": {
        "id": "chWRvDCNTELC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatgpt(\"Tell me how to train my dog to sit\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "WUmdcw0KTEOA",
        "outputId": "44c807f5-97e8-4ff3-c314-a09efeb777e0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "status code: 400\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama/engine/lamini.py\u001b[0m in \u001b[0;36mmake_web_request\u001b[0;34m(self, url, http_method, json)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api.powerml.co/v2/lamini/completions",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUserError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d65ab6ab592f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchatgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tell me how to train my dog to sit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama/runners/basic_model_runner.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Singleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0minput_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         output_objects = self.llm(\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama/engine/typed_lamini.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"output_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama/engine/lamini.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input, output_type, stop_tokens, model_name, enable_peft, random, max_tokens)\u001b[0m\n\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"completions\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_web_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# https://lamini-ai.github.io/API/data/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama/engine/lamini.py\u001b[0m in \u001b[0;36mmake_web_request\u001b[0;34m(self, url, http_method, json)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mUserError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"detail\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"UserError\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m503\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUserError\u001b[0m: Currently this user has support for base models: ['hf-internal-testing/tiny-random-gpt2', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-70m-v0', 'EleutherAI/pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-v1', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v1', 'EleutherAI/gpt-neo-125m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-160m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m', 'EleutherAI/neox-ckpt-pythia-160m', 'EleutherAI/neox-ckpt-pythia-160m-deduped-v1', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-410m-v0', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-410m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-410m', 'EleutherAI/neox-ckpt-pythia-410m-deduped-v1', 'cerebras/Cerebras-GPT-111M', 'cerebras/Cerebras-GPT-256M', 'meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-chat-hf']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e6IGf_HdTEQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R9JSOSK3TETo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cS63rO-zTEWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}