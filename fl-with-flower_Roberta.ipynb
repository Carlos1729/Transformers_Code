{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "853cd2ee195648c18347e55b8a812f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c826156b48247bfa0827bf526cd06e2",
              "IPY_MODEL_a06eed1af50449b39b2b252fb48e3006",
              "IPY_MODEL_bb06551293bf496a92f10d675c3cf2c9"
            ],
            "layout": "IPY_MODEL_e7dcf340ea904d25b49c3f513a39d32c"
          }
        },
        "4c826156b48247bfa0827bf526cd06e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7522d3614c2e4de5bfb74bd6e8b39016",
            "placeholder": "​",
            "style": "IPY_MODEL_db6e69b1ad0946fab949678490f76a3a",
            "value": "config.json: 100%"
          }
        },
        "a06eed1af50449b39b2b252fb48e3006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3ae3bc0515d4834b5331b421fc4549d",
            "max": 559,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3fa715db5624a74bcb38644a7a56341",
            "value": 559
          }
        },
        "bb06551293bf496a92f10d675c3cf2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28f42377e6104ea5abc94dda6ff6fc5a",
            "placeholder": "​",
            "style": "IPY_MODEL_91f8bf7dbe2b4892aed72c285916429b",
            "value": " 559/559 [00:00&lt;00:00, 38.0kB/s]"
          }
        },
        "e7dcf340ea904d25b49c3f513a39d32c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7522d3614c2e4de5bfb74bd6e8b39016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db6e69b1ad0946fab949678490f76a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3ae3bc0515d4834b5331b421fc4549d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3fa715db5624a74bcb38644a7a56341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28f42377e6104ea5abc94dda6ff6fc5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91f8bf7dbe2b4892aed72c285916429b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a7c332ae91a43d99502ca830aec7af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63d6b790e0414b3082c7110dba8e3a73",
              "IPY_MODEL_c4e3efd87b9e455eb760a5578f193eeb",
              "IPY_MODEL_3b8173e9ff2044edab4145b57c5e3dbe"
            ],
            "layout": "IPY_MODEL_5920d904f9764b0d8bdced56fe9c60b0"
          }
        },
        "63d6b790e0414b3082c7110dba8e3a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cba905551a94ebcba84d90b8a4891c7",
            "placeholder": "​",
            "style": "IPY_MODEL_ef8d0e4a81c646719e1ba72f6247a621",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "c4e3efd87b9e455eb760a5578f193eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a3da9053f084a61a51f9c9f3bcc5e1c",
            "max": 501003010,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5095360ff066416a95731e35de463733",
            "value": 501003010
          }
        },
        "3b8173e9ff2044edab4145b57c5e3dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5014cada067a4d99b2589f7fbf49017c",
            "placeholder": "​",
            "style": "IPY_MODEL_3e0a34aeee8c4fee8887040a1be129c1",
            "value": " 501M/501M [00:34&lt;00:00, 14.1MB/s]"
          }
        },
        "5920d904f9764b0d8bdced56fe9c60b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cba905551a94ebcba84d90b8a4891c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef8d0e4a81c646719e1ba72f6247a621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a3da9053f084a61a51f9c9f3bcc5e1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5095360ff066416a95731e35de463733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5014cada067a4d99b2589f7fbf49017c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e0a34aeee8c4fee8887040a1be129c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70f843f0493040659a975e0ed729b786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19462401a7624f779ed38e4362f67f15",
              "IPY_MODEL_97f9003dff514e9fbbf63929392de97a",
              "IPY_MODEL_21427030f7974e8385584182fe592e54"
            ],
            "layout": "IPY_MODEL_b8d90a46a07b4b8b8757f76f72ef91e8"
          }
        },
        "19462401a7624f779ed38e4362f67f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e223799984a4253a293f0fb1b3dcae7",
            "placeholder": "​",
            "style": "IPY_MODEL_da9b72c1cd2d43018634a0f081a5428e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "97f9003dff514e9fbbf63929392de97a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_426c2e893ea042a6837a02f5f8863b47",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ae0efa0798845669564b7cd594a956b",
            "value": 25
          }
        },
        "21427030f7974e8385584182fe592e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2226c64d7816452a926af9e9ba114b62",
            "placeholder": "​",
            "style": "IPY_MODEL_371d0efada9d4ec9a3400f2ee42fc683",
            "value": " 25.0/25.0 [00:00&lt;00:00, 1.47kB/s]"
          }
        },
        "b8d90a46a07b4b8b8757f76f72ef91e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e223799984a4253a293f0fb1b3dcae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da9b72c1cd2d43018634a0f081a5428e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "426c2e893ea042a6837a02f5f8863b47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ae0efa0798845669564b7cd594a956b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2226c64d7816452a926af9e9ba114b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "371d0efada9d4ec9a3400f2ee42fc683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be9d0601810942bd85cde2c0a6b5a337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0260b9da8c2d4f14ad7cfea6b26d80ea",
              "IPY_MODEL_ee1df8a1b1924adea5a4f4e012cb7a4a",
              "IPY_MODEL_e8ccd460d3c944f79c50d654a2c9f407"
            ],
            "layout": "IPY_MODEL_c3d1ecac2f5e4b6fa50f62b09fcdfb68"
          }
        },
        "0260b9da8c2d4f14ad7cfea6b26d80ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f09c4512eb374733a5b4f7dbfd34f250",
            "placeholder": "​",
            "style": "IPY_MODEL_d79d7f83fa7446e285f5693fbab1d225",
            "value": "vocab.json: 100%"
          }
        },
        "ee1df8a1b1924adea5a4f4e012cb7a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_137aa2cdb1894c71a83913bd835457b5",
            "max": 798293,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc15d6fbc4344db49611bd713fe0aa56",
            "value": 798293
          }
        },
        "e8ccd460d3c944f79c50d654a2c9f407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5d0b7b0d3af4cf9a0ef64002c2fc390",
            "placeholder": "​",
            "style": "IPY_MODEL_be0cece097f24f2d9469b8ef70eddcbc",
            "value": " 798k/798k [00:00&lt;00:00, 3.05MB/s]"
          }
        },
        "c3d1ecac2f5e4b6fa50f62b09fcdfb68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f09c4512eb374733a5b4f7dbfd34f250": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d79d7f83fa7446e285f5693fbab1d225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "137aa2cdb1894c71a83913bd835457b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc15d6fbc4344db49611bd713fe0aa56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5d0b7b0d3af4cf9a0ef64002c2fc390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be0cece097f24f2d9469b8ef70eddcbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d891ff4ef14464b976425458c6e8813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5896370c77746449d941b63be529ab9",
              "IPY_MODEL_652a2c6542154dcdb56ba1180b4b9183",
              "IPY_MODEL_0d9abcf29b7b4f98b630879a67dd54ec"
            ],
            "layout": "IPY_MODEL_1b7985e005394603a922ca7b10e26bcb"
          }
        },
        "b5896370c77746449d941b63be529ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b2d0eb6a6414575abaaa2779d461862",
            "placeholder": "​",
            "style": "IPY_MODEL_6656d269b12f4de78a61e4cc2b5ccd60",
            "value": "merges.txt: 100%"
          }
        },
        "652a2c6542154dcdb56ba1180b4b9183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72075b437cf949f0b734f94e48ae5a8d",
            "max": 456356,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f93eae59ff143f3b1afdf261acf7883",
            "value": 456356
          }
        },
        "0d9abcf29b7b4f98b630879a67dd54ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f792aae21d54628896ba878c3c34f70",
            "placeholder": "​",
            "style": "IPY_MODEL_f40391f0b7014aaaaffa7806125d3d5d",
            "value": " 456k/456k [00:00&lt;00:00, 20.3MB/s]"
          }
        },
        "1b7985e005394603a922ca7b10e26bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b2d0eb6a6414575abaaa2779d461862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6656d269b12f4de78a61e4cc2b5ccd60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72075b437cf949f0b734f94e48ae5a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f93eae59ff143f3b1afdf261acf7883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f792aae21d54628896ba878c3c34f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f40391f0b7014aaaaffa7806125d3d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b284b08d418a4f2a810a623e5595dc5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36848e07182444109e228801b299b19f",
              "IPY_MODEL_f7e6459f008e45508d56e907ae388fea",
              "IPY_MODEL_284aeb499f484ea0932c6102f4737f8d"
            ],
            "layout": "IPY_MODEL_2a16a1ac40194da59fc04f883ad6e246"
          }
        },
        "36848e07182444109e228801b299b19f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8424828a061046c584c57b27c5d1bc20",
            "placeholder": "​",
            "style": "IPY_MODEL_cce6c0cc66fa43a8a9c24be1cb6c8eb0",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "f7e6459f008e45508d56e907ae388fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f40f44d83f994bb180e2635eeb7504d5",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b2da52e9924481d8f4968526bc95c4e",
            "value": 239
          }
        },
        "284aeb499f484ea0932c6102f4737f8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be1b121f09dc40c9b114f223a062c57a",
            "placeholder": "​",
            "style": "IPY_MODEL_4ca4fbb687a34408bf8a17783ae796e9",
            "value": " 239/239 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "2a16a1ac40194da59fc04f883ad6e246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8424828a061046c584c57b27c5d1bc20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cce6c0cc66fa43a8a9c24be1cb6c8eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f40f44d83f994bb180e2635eeb7504d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b2da52e9924481d8f4968526bc95c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be1b121f09dc40c9b114f223a062c57a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ca4fbb687a34408bf8a17783ae796e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44f43e5edd8c45139ba5a867127a5622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4351a27b8254443dbd2a7b7a26e5109b",
              "IPY_MODEL_156c77c92dd44b15a197b1edcc5dcc29",
              "IPY_MODEL_ac85742ee7774cdf845383268bcbeb95"
            ],
            "layout": "IPY_MODEL_2482b775599444cc81d6aa50079b9a78"
          }
        },
        "4351a27b8254443dbd2a7b7a26e5109b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5109def17fb847fd8152cc40d35e8aa4",
            "placeholder": "​",
            "style": "IPY_MODEL_710c2fb4b5e04cb6b91af93215e0cc15",
            "value": "Map: 100%"
          }
        },
        "156c77c92dd44b15a197b1edcc5dcc29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b06decd32de43c3a17efa7ba9f77117",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36821904d46b4377b29c38fb21078133",
            "value": 25000
          }
        },
        "ac85742ee7774cdf845383268bcbeb95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efd7c0a1485b45939372365a19884dd0",
            "placeholder": "​",
            "style": "IPY_MODEL_08b25a2f096143a894d6f8bb0d8fcadb",
            "value": " 25000/25000 [00:22&lt;00:00, 1232.31 examples/s]"
          }
        },
        "2482b775599444cc81d6aa50079b9a78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5109def17fb847fd8152cc40d35e8aa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "710c2fb4b5e04cb6b91af93215e0cc15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b06decd32de43c3a17efa7ba9f77117": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36821904d46b4377b29c38fb21078133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efd7c0a1485b45939372365a19884dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b25a2f096143a894d6f8bb0d8fcadb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2f65b5313834532ae771a3eade2c97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_387406865a3c4a42802ce735d5d89507",
              "IPY_MODEL_8cb1ba56c8474f1dadb24647e3a086df",
              "IPY_MODEL_6711d623d79845d2bd3128f34c7bd93f"
            ],
            "layout": "IPY_MODEL_27159fe336cc4433af5ab0be9d347798"
          }
        },
        "387406865a3c4a42802ce735d5d89507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdef3961e2eb4936b549925cba1f8bb5",
            "placeholder": "​",
            "style": "IPY_MODEL_5b0600356bec4f1a9abf4d213808eb0f",
            "value": "Map: 100%"
          }
        },
        "8cb1ba56c8474f1dadb24647e3a086df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f4ae3ea3f53489ea9a4f15be85216f3",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc1853c6aaa6492cb710383ae640cd38",
            "value": 25000
          }
        },
        "6711d623d79845d2bd3128f34c7bd93f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed33aed3757c4bed834e4b0bc2c4ed5f",
            "placeholder": "​",
            "style": "IPY_MODEL_0ecdd713ef894532862895b6834af433",
            "value": " 25000/25000 [00:24&lt;00:00, 1087.59 examples/s]"
          }
        },
        "27159fe336cc4433af5ab0be9d347798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdef3961e2eb4936b549925cba1f8bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b0600356bec4f1a9abf4d213808eb0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f4ae3ea3f53489ea9a4f15be85216f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc1853c6aaa6492cb710383ae640cd38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed33aed3757c4bed834e4b0bc2c4ed5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ecdd713ef894532862895b6834af433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carlos1729/Transformers_Code/blob/main/fl-with-flower_Roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Federated Learning using Hugging Face and Flower\n",
        "\n",
        "This tutorial will show how to leverage Hugging Face to federate the training of language models over multiple clients using [Flower](https://flower.dev/). More specifically, we will fine-tune a pre-trained Transformer model (alBERT) for sequence classification over a dataset of IMDB ratings. The end goal is to detect if a movie rating is positive or negative.\n"
      ],
      "metadata": {
        "id": "ESpKTVP3F_Xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n",
        "\n",
        "For this tutorial we will need `datasets`, `flwr['simulation']`(here we use the extra 'simulation' dependencies from Flower as we will simulated the federated setting inside Google Colab), `torch`, and `transformers`."
      ],
      "metadata": {
        "id": "hcUWBC4ih-mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate flwr[\"simulation\"] torch transformers"
      ],
      "metadata": {
        "id": "zBuj5kSif2yt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76dc43b-7f2c-4847-b200-f8cf8ef341d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/521.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/521.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flwr[simulation]\n",
            "  Downloading flwr-1.6.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.2/219.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: cryptography<42.0.0,>=41.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (41.0.7)\n",
            "Requirement already satisfied: grpcio!=1.52.0,<2.0.0,>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (1.59.3)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr[simulation])\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (3.20.3)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr[simulation])\n",
            "  Downloading pycryptodome-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (1.10.13)\n",
            "Collecting ray==2.6.3 (from flwr[simulation])\n",
            "  Downloading ray-2.6.3-cp310-cp310-manylinux2014_x86_64.whl (56.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (1.0.7)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<42.0.0,>=41.0.2->flwr[simulation]) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.2->flwr[simulation]) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.6.3->flwr[simulation]) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.6.3->flwr[simulation]) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.6.3->flwr[simulation]) (0.13.2)\n",
            "Installing collected packages: pycryptodome, pyarrow-hotfix, iterators, dill, responses, multiprocess, flwr, ray, datasets, evaluate\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 evaluate-0.4.1 flwr-1.6.0 iterators-0.0.2 multiprocess-0.70.15 pyarrow-hotfix-0.6 pycryptodome-3.19.0 ray-2.6.3 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now import the relevant modules."
      ],
      "metadata": {
        "id": "Q5I0ZUC4hpua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import flwr as fl\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "from evaluate import load as load_metric\n",
        "\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from transformers import logging"
      ],
      "metadata": {
        "id": "IhNwuY-Oefau"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will set some global variables and disable some of the logging to clear out our output."
      ],
      "metadata": {
        "id": "J-gZqELEhsun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT = \"textattack/roberta-base-imdb\"  # transformer model checkpoint\n",
        "# CHECKPOINT = \"roberta-base\"  # transformer model checkpoint\n",
        "NUM_CLIENTS = 2\n",
        "NUM_ROUNDS = 3"
      ],
      "metadata": {
        "id": "AH0Sx53Rehjc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Hugging Face workflow\n",
        "\n",
        "### Handling the data\n",
        "\n",
        "To fetch the IMDB dataset, we will use Hugging Face's `datasets` library. We then need to tokenize the data and create `PyTorch` dataloaders, this is all done in the `load_data` function:"
      ],
      "metadata": {
        "id": "aI21VQX-GRSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \"\"\"Load IMDB data (training and eval)\"\"\"\n",
        "    raw_datasets = load_dataset(\"imdb\")\n",
        "    raw_datasets = raw_datasets.shuffle(seed=42)\n",
        "\n",
        "    # remove unnecessary data split\n",
        "    del raw_datasets[\"unsupervised\"]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "    # Select 20 random samples to reduce the computation cost\n",
        "    train_population = random.sample(range(len(raw_datasets[\"train\"])), 20)\n",
        "    test_population = random.sample(range(len(raw_datasets[\"test\"])), 20)\n",
        "\n",
        "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].select(train_population)\n",
        "    tokenized_datasets[\"test\"] = tokenized_datasets[\"test\"].select(test_population)\n",
        "\n",
        "    tokenized_datasets = tokenized_datasets.remove_columns(\"text\")\n",
        "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    trainloader = DataLoader(\n",
        "        tokenized_datasets[\"train\"],\n",
        "        shuffle=True,\n",
        "        batch_size=32,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    testloader = DataLoader(\n",
        "        tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "    return trainloader, testloader"
      ],
      "metadata": {
        "id": "06-OMJtvekAB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and testing the model\n",
        "\n",
        "Once we have a way of creating our trainloader and testloader, we can take care of the training and testing. This is very similar to any `PyTorch` training or testing loop:"
      ],
      "metadata": {
        "id": "s1UtfzMFGVKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, trainloader, epochs):\n",
        "    optimizer = AdamW(net.parameters(), lr=5e-5)\n",
        "    net.train()\n",
        "    for _ in range(epochs):\n",
        "        for batch in trainloader:\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "            outputs = net(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    loss = 0\n",
        "    net.eval()\n",
        "    for batch in testloader:\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = net(**batch)\n",
        "        logits = outputs.logits\n",
        "        loss += outputs.loss.item()\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = metric.compute()[\"accuracy\"]\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "szd1PmUbem1v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the model itself\n",
        "\n",
        "To create the model itself, we will just load the pre-trained alBERT model using Hugging Face’s `AutoModelForSequenceClassification` :"
      ],
      "metadata": {
        "id": "rVbWtgQLGhFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = AutoModelForSequenceClassification.from_pretrained(\n",
        "    CHECKPOINT, num_labels=2\n",
        ").to(DEVICE)"
      ],
      "metadata": {
        "id": "qeiueaYKGiBf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "853cd2ee195648c18347e55b8a812f79",
            "4c826156b48247bfa0827bf526cd06e2",
            "a06eed1af50449b39b2b252fb48e3006",
            "bb06551293bf496a92f10d675c3cf2c9",
            "e7dcf340ea904d25b49c3f513a39d32c",
            "7522d3614c2e4de5bfb74bd6e8b39016",
            "db6e69b1ad0946fab949678490f76a3a",
            "d3ae3bc0515d4834b5331b421fc4549d",
            "b3fa715db5624a74bcb38644a7a56341",
            "28f42377e6104ea5abc94dda6ff6fc5a",
            "91f8bf7dbe2b4892aed72c285916429b",
            "0a7c332ae91a43d99502ca830aec7af9",
            "63d6b790e0414b3082c7110dba8e3a73",
            "c4e3efd87b9e455eb760a5578f193eeb",
            "3b8173e9ff2044edab4145b57c5e3dbe",
            "5920d904f9764b0d8bdced56fe9c60b0",
            "9cba905551a94ebcba84d90b8a4891c7",
            "ef8d0e4a81c646719e1ba72f6247a621",
            "2a3da9053f084a61a51f9c9f3bcc5e1c",
            "5095360ff066416a95731e35de463733",
            "5014cada067a4d99b2589f7fbf49017c",
            "3e0a34aeee8c4fee8887040a1be129c1"
          ]
        },
        "outputId": "7ee28069-360d-4adb-87e3-47ad52578852"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/559 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "853cd2ee195648c18347e55b8a812f79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a7c332ae91a43d99502ca830aec7af9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federating the example\n",
        "\n",
        "The idea behind Federated Learning is to train a model between multiple clients and a server without having to share any data. This is done by letting each client train the model locally on its data and send its parameters back to the server, which then aggregates all the clients’ parameters together using a predefined strategy. This process is made very simple by using the [Flower](https://github.com/adap/flower) framework. If you want a more complete overview, be sure to check out this guide: [What is Federated Learning?](https://flower.dev/docs/tutorial/Flower-0-What-is-FL.html)\n",
        "\n",
        "### Creating the IMDBClient\n",
        "\n",
        "To federate our example to multiple clients, we first need to write our Flower client class (inheriting from `flwr.client.NumPyClient`). This is very easy, as our model is a standard `PyTorch` model:"
      ],
      "metadata": {
        "id": "Mx95k0TUGtSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBClient(fl.client.NumPyClient):\n",
        "    def __init__(self, net, trainloader, testloader):\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.testloader = testloader\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        params_dict = zip(self.net.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        self.net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.set_parameters(parameters)\n",
        "        print(\"Training Started...\")\n",
        "        train(self.net, self.trainloader, epochs=1)\n",
        "        print(\"Training Finished.\")\n",
        "        return self.get_parameters(config={}), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.set_parameters(parameters)\n",
        "        loss, accuracy = test(self.net, self.testloader)\n",
        "        return float(loss), len(self.testloader), {\"accuracy\": float(accuracy), \"loss\": float(loss)}"
      ],
      "metadata": {
        "id": "-sSuLWYzeuPC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `get_parameters` function lets the server get the client's parameters. Inversely, the `set_parameters` function allows the server to send its parameters to the client. Finally, the `fit` function trains the model locally for the client, and the `evaluate` function tests the model locally and returns the relevant metrics."
      ],
      "metadata": {
        "id": "PTdzUBpkG3jE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating the clients\n",
        "\n",
        "In order to simulate the federated setting we need to provide a way to instantiate clients for our simulation. Here, it is very simple as every client will hold the same piece of data (this is not realistic, it is just used here for simplicity sakes)."
      ],
      "metadata": {
        "id": "kZDZ3KUaGapq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader, testloader = load_data()\n",
        "def client_fn(cid):\n",
        "  return IMDBClient(net, trainloader, testloader)"
      ],
      "metadata": {
        "id": "y9A11kmafSwX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "70f843f0493040659a975e0ed729b786",
            "19462401a7624f779ed38e4362f67f15",
            "97f9003dff514e9fbbf63929392de97a",
            "21427030f7974e8385584182fe592e54",
            "b8d90a46a07b4b8b8757f76f72ef91e8",
            "4e223799984a4253a293f0fb1b3dcae7",
            "da9b72c1cd2d43018634a0f081a5428e",
            "426c2e893ea042a6837a02f5f8863b47",
            "3ae0efa0798845669564b7cd594a956b",
            "2226c64d7816452a926af9e9ba114b62",
            "371d0efada9d4ec9a3400f2ee42fc683",
            "be9d0601810942bd85cde2c0a6b5a337",
            "0260b9da8c2d4f14ad7cfea6b26d80ea",
            "ee1df8a1b1924adea5a4f4e012cb7a4a",
            "e8ccd460d3c944f79c50d654a2c9f407",
            "c3d1ecac2f5e4b6fa50f62b09fcdfb68",
            "f09c4512eb374733a5b4f7dbfd34f250",
            "d79d7f83fa7446e285f5693fbab1d225",
            "137aa2cdb1894c71a83913bd835457b5",
            "bc15d6fbc4344db49611bd713fe0aa56",
            "b5d0b7b0d3af4cf9a0ef64002c2fc390",
            "be0cece097f24f2d9469b8ef70eddcbc",
            "4d891ff4ef14464b976425458c6e8813",
            "b5896370c77746449d941b63be529ab9",
            "652a2c6542154dcdb56ba1180b4b9183",
            "0d9abcf29b7b4f98b630879a67dd54ec",
            "1b7985e005394603a922ca7b10e26bcb",
            "8b2d0eb6a6414575abaaa2779d461862",
            "6656d269b12f4de78a61e4cc2b5ccd60",
            "72075b437cf949f0b734f94e48ae5a8d",
            "7f93eae59ff143f3b1afdf261acf7883",
            "8f792aae21d54628896ba878c3c34f70",
            "f40391f0b7014aaaaffa7806125d3d5d",
            "b284b08d418a4f2a810a623e5595dc5f",
            "36848e07182444109e228801b299b19f",
            "f7e6459f008e45508d56e907ae388fea",
            "284aeb499f484ea0932c6102f4737f8d",
            "2a16a1ac40194da59fc04f883ad6e246",
            "8424828a061046c584c57b27c5d1bc20",
            "cce6c0cc66fa43a8a9c24be1cb6c8eb0",
            "f40f44d83f994bb180e2635eeb7504d5",
            "5b2da52e9924481d8f4968526bc95c4e",
            "be1b121f09dc40c9b114f223a062c57a",
            "4ca4fbb687a34408bf8a17783ae796e9",
            "44f43e5edd8c45139ba5a867127a5622",
            "4351a27b8254443dbd2a7b7a26e5109b",
            "156c77c92dd44b15a197b1edcc5dcc29",
            "ac85742ee7774cdf845383268bcbeb95",
            "2482b775599444cc81d6aa50079b9a78",
            "5109def17fb847fd8152cc40d35e8aa4",
            "710c2fb4b5e04cb6b91af93215e0cc15",
            "4b06decd32de43c3a17efa7ba9f77117",
            "36821904d46b4377b29c38fb21078133",
            "efd7c0a1485b45939372365a19884dd0",
            "08b25a2f096143a894d6f8bb0d8fcadb",
            "c2f65b5313834532ae771a3eade2c97d",
            "387406865a3c4a42802ce735d5d89507",
            "8cb1ba56c8474f1dadb24647e3a086df",
            "6711d623d79845d2bd3128f34c7bd93f",
            "27159fe336cc4433af5ab0be9d347798",
            "fdef3961e2eb4936b549925cba1f8bb5",
            "5b0600356bec4f1a9abf4d213808eb0f",
            "4f4ae3ea3f53489ea9a4f15be85216f3",
            "dc1853c6aaa6492cb710383ae640cd38",
            "ed33aed3757c4bed834e4b0bc2c4ed5f",
            "0ecdd713ef894532862895b6834af433"
          ]
        },
        "outputId": "978d9334-ad12-4e4d-f914-3f6f6de99080"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70f843f0493040659a975e0ed729b786"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be9d0601810942bd85cde2c0a6b5a337"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d891ff4ef14464b976425458c6e8813"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b284b08d418a4f2a810a623e5595dc5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44f43e5edd8c45139ba5a867127a5622"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2f65b5313834532ae771a3eade2c97d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting the simulation\n",
        "\n",
        "We now have all the elements to start our simulation. The `weighted_average` function is there to provide a way to aggregate the metrics distributed amongst the clients (basically to display a nice average accuracy at the end of the training). We then define our strategy (here `FedAvg`, which will aggregate the clients weights by doing an average).\n",
        "\n",
        "Finally, `start_simulation` is used to start the training."
      ],
      "metadata": {
        "id": "Y7dcCPKDjaFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_average(metrics):\n",
        "  accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "  losses = [num_examples * m[\"loss\"] for num_examples, m in metrics]\n",
        "  examples = [num_examples for num_examples, _ in metrics]\n",
        "  return {\"accuracy\": sum(accuracies) / sum(examples), \"loss\": sum(losses) / sum(examples)}\n",
        "\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1.0,\n",
        "    fraction_evaluate=1.0,\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,\n",
        ")\n",
        "\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
        "    strategy=strategy,\n",
        "    client_resources={\"num_cpus\": 8, \"num_gpus\": 2},\n",
        "    ray_init_args={\"log_to_driver\": False, \"num_cpus\": 8, \"num_gpus\": 2}\n",
        ")"
      ],
      "metadata": {
        "id": "s6Jsw70Qe_yA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eaf278f-4103-4661-a525-c390998bfe18"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO flwr 2023-12-07 12:39:43,115 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
            "2023-12-07 12:39:48,906\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2023-12-07 12:39:51,578 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'GPU': 2.0, 'memory': 7970230272.0, 'object_store_memory': 3985115136.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'GPU': 2.0, 'memory': 7970230272.0, 'object_store_memory': 3985115136.0}\n",
            "INFO flwr 2023-12-07 12:39:51,582 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
            "INFO:flwr:Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
            "INFO flwr 2023-12-07 12:39:51,584 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 2}\n",
            "INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 2}\n",
            "INFO flwr 2023-12-07 12:39:51,629 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
            "INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
            "INFO flwr 2023-12-07 12:39:51,636 | server.py:89 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-12-07 12:39:51,639 | server.py:276 | Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "INFO flwr 2023-12-07 12:40:10,707 | server.py:280 | Received initial parameters from one random client\n",
            "INFO:flwr:Received initial parameters from one random client\n",
            "INFO flwr 2023-12-07 12:40:10,716 | server.py:91 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "INFO flwr 2023-12-07 12:40:10,718 | server.py:104 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2023-12-07 12:40:10,721 | server.py:222 | fit_round 1: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 2 clients (out of 2)\n",
            "ERROR flwr 2023-12-07 12:40:17,857 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=6179, ip=172.28.0.12, actor_id=c5326bb5bb7a85387508a03401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7bda3786c730>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\n",
            "    return maybe_call_fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "  File \"<ipython-input-24-4a72afdb44ba>\", line 18, in fit\n",
            "  File \"<ipython-input-22-52920450db6a>\", line 7, in train\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1198, in forward\n",
            "    outputs = self.roberta(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
            "    self_attention_outputs = self.attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
            "    self_outputs = self.self(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 270, in forward\n",
            "    attention_probs = self.dropout(attention_probs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 114.81 MiB is free. Process 3207 has 1.63 GiB memory in use. Process 43439 has 13.00 GiB memory in use. Of the allocated memory 12.80 GiB is allocated by PyTorch, and 83.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=6179, ip=172.28.0.12, actor_id=c5326bb5bb7a85387508a03401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7bda3786c730>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 1 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 72, in run\\n    job_results = job_fn(client)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\\n    return maybe_call_fit(\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\\n    return client.fit(fit_ins)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\\n  File \"<ipython-input-24-4a72afdb44ba>\", line 18, in fit\\n  File \"<ipython-input-22-52920450db6a>\", line 7, in train\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1198, in forward\\n    outputs = self.roberta(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\\n    encoder_outputs = self.encoder(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\\n    layer_outputs = layer_module(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\\n    self_attention_outputs = self.attention(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\\n    self_outputs = self.self(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 270, in forward\\n    attention_probs = self.dropout(attention_probs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\\n    return F.dropout(input, self.p, self.training, self.inplace)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 114.81 MiB is free. Process 3207 has 1.63 GiB memory in use. Process 43439 has 13.00 GiB memory in use. Of the allocated memory 12.80 GiB is allocated by PyTorch, and 83.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n',)\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=6179, ip=172.28.0.12, actor_id=c5326bb5bb7a85387508a03401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7bda3786c730>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\n",
            "    return maybe_call_fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "  File \"<ipython-input-24-4a72afdb44ba>\", line 18, in fit\n",
            "  File \"<ipython-input-22-52920450db6a>\", line 7, in train\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1198, in forward\n",
            "    outputs = self.roberta(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
            "    self_attention_outputs = self.attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
            "    self_outputs = self.self(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 270, in forward\n",
            "    attention_probs = self.dropout(attention_probs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 114.81 MiB is free. Process 3207 has 1.63 GiB memory in use. Process 43439 has 13.00 GiB memory in use. Of the allocated memory 12.80 GiB is allocated by PyTorch, and 83.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=6179, ip=172.28.0.12, actor_id=c5326bb5bb7a85387508a03401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7bda3786c730>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 1 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 72, in run\\n    job_results = job_fn(client)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\\n    return maybe_call_fit(\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\\n    return client.fit(fit_ins)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\\n  File \"<ipython-input-24-4a72afdb44ba>\", line 18, in fit\\n  File \"<ipython-input-22-52920450db6a>\", line 7, in train\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1198, in forward\\n    outputs = self.roberta(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\\n    encoder_outputs = self.encoder(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\\n    layer_outputs = layer_module(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\\n    self_attention_outputs = self.attention(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\\n    self_outputs = self.self(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 270, in forward\\n    attention_probs = self.dropout(attention_probs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\\n    return F.dropout(input, self.p, self.training, self.inplace)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 114.81 MiB is free. Process 3207 has 1.63 GiB memory in use. Process 43439 has 13.00 GiB memory in use. Of the allocated memory 12.80 GiB is allocated by PyTorch, and 83.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n',)\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:17,862 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:17,872 | ray_client_proxy.py:146 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=6179, ip=172.28.0.12, actor_id=c5326bb5bb7a85387508a03401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7bda3786c730>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\n",
            "    return maybe_call_fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "  File \"<ipython-input-24-4a72afdb44ba>\", line 18, in fit\n",
            "  File \"<ipython-input-22-52920450db6a>\", line 7, in train\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1198, in forward\n",
            "    outputs = self.roberta(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
            "    self_attention_outputs = self.attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
            "    self_outputs = self.self(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 270, in forward\n",
            "    attention_probs = self.dropout(attention_probs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 114.81 MiB is free. Process 3207 has 1.63 GiB memory in use. Process 43439 has 13.00 GiB memory in use. Of the allocated memory 12.80 GiB is allocated by PyTorch, and 83.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=6179, ip=172.28.0.12, actor_id=c5326bb5bb7a85387508a03401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7bda3786c730>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 1 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 72, in run\\n    job_results = job_fn(client)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\\n    return maybe_call_fit(\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\\n    return client.fit(fit_ins)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\\n  File \"<ipython-input-24-4a72afdb44ba>\", line 18, in fit\\n  File \"<ipython-input-22-52920450db6a>\", line 7, in train\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1198, in forward\\n    outputs = self.roberta(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\\n    encoder_outputs = self.encoder(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\\n    layer_outputs = layer_module(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\\n    self_attention_outputs = self.attention(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\\n    self_outputs = self.self(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 270, in forward\\n    attention_probs = self.dropout(attention_probs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\\n    return F.dropout(input, self.p, self.training, self.inplace)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 114.81 MiB is free. Process 3207 has 1.63 GiB memory in use. Process 43439 has 13.00 GiB memory in use. Of the allocated memory 12.80 GiB is allocated by PyTorch, and 83.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n',)\n",
            "ERROR:flwr:\u001b[36mray::DefaultActor.run()\u001b[39m (pid=6179, ip=172.28.0.12, actor_id=c5326bb5bb7a85387508a03401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7bda3786c730>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\n",
            "    return maybe_call_fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "  File \"<ipython-input-24-4a72afdb44ba>\", line 18, in fit\n",
            "  File \"<ipython-input-22-52920450db6a>\", line 7, in train\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1198, in forward\n",
            "    outputs = self.roberta(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
            "    self_attention_outputs = self.attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
            "    self_outputs = self.self(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 270, in forward\n",
            "    attention_probs = self.dropout(attention_probs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 114.81 MiB is free. Process 3207 has 1.63 GiB memory in use. Process 43439 has 13.00 GiB memory in use. Of the allocated memory 12.80 GiB is allocated by PyTorch, and 83.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=6179, ip=172.28.0.12, actor_id=c5326bb5bb7a85387508a03401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7bda3786c730>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 1 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 72, in run\\n    job_results = job_fn(client)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\\n    return maybe_call_fit(\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\\n    return client.fit(fit_ins)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\\n  File \"<ipython-input-24-4a72afdb44ba>\", line 18, in fit\\n  File \"<ipython-input-22-52920450db6a>\", line 7, in train\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1198, in forward\\n    outputs = self.roberta(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\\n    encoder_outputs = self.encoder(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\\n    layer_outputs = layer_module(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\\n    self_attention_outputs = self.attention(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\\n    self_outputs = self.self(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 270, in forward\\n    attention_probs = self.dropout(attention_probs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\\n    return F.dropout(input, self.p, self.training, self.inplace)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 240.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 114.81 MiB is free. Process 3207 has 1.63 GiB memory in use. Process 43439 has 13.00 GiB memory in use. Of the allocated memory 12.80 GiB is allocated by PyTorch, and 83.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n',)\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:17,880 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-07 12:40:17,882 | server.py:236 | fit_round 1 received 0 results and 2 failures\n",
            "DEBUG:flwr:fit_round 1 received 0 results and 2 failures\n",
            "DEBUG flwr 2023-12-07 12:40:17,885 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 2 clients (out of 2)\n",
            "ERROR flwr 2023-12-07 12:40:20,800 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:20,803 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2023-12-07 12:40:20,810 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:20,812 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-07 12:40:20,815 | server.py:187 | evaluate_round 1 received 0 results and 2 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 0 results and 2 failures\n",
            "DEBUG flwr 2023-12-07 12:40:20,817 | server.py:222 | fit_round 2: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 2 clients (out of 2)\n",
            "ERROR flwr 2023-12-07 12:40:24,262 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:24,267 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2023-12-07 12:40:24,273 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:24,275 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-07 12:40:24,282 | server.py:236 | fit_round 2 received 0 results and 2 failures\n",
            "DEBUG:flwr:fit_round 2 received 0 results and 2 failures\n",
            "DEBUG flwr 2023-12-07 12:40:24,298 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flwr:evaluate_round 2: strategy sampled 2 clients (out of 2)\n",
            "ERROR flwr 2023-12-07 12:40:27,425 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:27,432 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2023-12-07 12:40:27,433 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:27,444 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-07 12:40:27,448 | server.py:187 | evaluate_round 2 received 0 results and 2 failures\n",
            "DEBUG:flwr:evaluate_round 2 received 0 results and 2 failures\n",
            "DEBUG flwr 2023-12-07 12:40:27,453 | server.py:222 | fit_round 3: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flwr:fit_round 3: strategy sampled 2 clients (out of 2)\n",
            "ERROR flwr 2023-12-07 12:40:30,188 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:30,195 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2023-12-07 12:40:30,202 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:30,205 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-07 12:40:30,211 | server.py:236 | fit_round 3 received 0 results and 2 failures\n",
            "DEBUG:flwr:fit_round 3 received 0 results and 2 failures\n",
            "DEBUG flwr 2023-12-07 12:40:30,213 | server.py:173 | evaluate_round 3: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flwr:evaluate_round 3: strategy sampled 2 clients (out of 2)\n",
            "ERROR flwr 2023-12-07 12:40:33,140 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:33,148 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-07 12:40:33,152 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2023-12-07 12:40:33,158 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: bc78eba74dc606237e63d77d368cd6f803106db29c3a2f6b92db5993) where the task (actor ID: c5326bb5bb7a85387508a03401000000, name=DefaultActor.__init__, pid=6179, memory used=1.40GB) was running was 12.10GB / 12.68GB (0.954511), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-22b35510b4276f8b87a686e0ed5e44d94db2b6d1733b7d614bd164cb*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "358\t7.69\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-708fbc58-b2e6...\n",
            "6179\t1.40\tray::DefaultActor\n",
            "1071\t0.62\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:de59e7032a0f5e814...\n",
            "104\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "6087\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "6029\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "6028\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "6089\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "55\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "7\t0.03\t/tools/node/bin/node /datalab/web/app.js\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-07 12:40:33,171 | server.py:187 | evaluate_round 3 received 0 results and 2 failures\n",
            "DEBUG:flwr:evaluate_round 3 received 0 results and 2 failures\n",
            "INFO flwr 2023-12-07 12:40:33,203 | server.py:153 | FL finished in 22.482044653999992\n",
            "INFO:flwr:FL finished in 22.482044653999992\n",
            "INFO flwr 2023-12-07 12:40:33,206 | app.py:226 | app_fit: losses_distributed []\n",
            "INFO:flwr:app_fit: losses_distributed []\n",
            "INFO flwr 2023-12-07 12:40:33,209 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
            "INFO:flwr:app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-12-07 12:40:33,211 | app.py:228 | app_fit: metrics_distributed {}\n",
            "INFO:flwr:app_fit: metrics_distributed {}\n",
            "INFO flwr 2023-12-07 12:40:33,214 | app.py:229 | app_fit: losses_centralized []\n",
            "INFO:flwr:app_fit: losses_centralized []\n",
            "INFO flwr 2023-12-07 12:40:33,216 | app.py:230 | app_fit: metrics_centralized {}\n",
            "INFO:flwr:app_fit: metrics_centralized {}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this is a very basic example, and a lot can be added or modified, it was just to showcase how simply we could federate a Hugging Face workflow using Flower. The number of clients and the data samples are intentionally very small in order to quickly run inside Colab, but keep in mind that everything can be tweaked and extended."
      ],
      "metadata": {
        "id": "YaIbuJ_xmsxk"
      }
    }
  ]
}